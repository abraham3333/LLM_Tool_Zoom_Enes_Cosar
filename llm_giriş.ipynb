{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GROQ: Yapay Zeka için Devrim Niteliğinde Bir Donanım Çözümü\n",
    "\n",
    "GROQ, Google TPU'nun mucidi Jonathan Ross tarafından geliştirilen ve LLM'leri çalıştırmak için özel olarak tasarlanmış bir Language Processing Unit (LPU) teknolojisidir.\n",
    "\n",
    "## Öne Çıkan Özellikleri:\n",
    "- Saniyede 500 token üretebilen olağanüstü hız : https://groq.com/pricing/\n",
    "- Çok düşük gecikme süresi ile gerçek zamanlı etkileşim  \n",
    "- GPU'lara göre daha yüksek enerji verimliliği\n",
    "- Mixtral, DeepSeek, Llama 3 gibi açık kaynak modelleri destekler\n",
    "- OpenAI API uyumluluğu ile kolay entegrasyon\n",
    "\n",
    "GROQ, özellikle düşük gecikme süresi gerektiren AI uygulamaları için ideal bir çözümdür ve gelecekte GPU'lara ciddi bir rakip olma potansiyeline sahiptir.\n",
    "\n",
    "https://medium.com/@leosorge/groq-lpu-language-processing-units-for-a-large-language-model-based-infrastructure-ba608cd4b927"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LinkedIn image](https://media.licdn.com/dms/image/v2/D4D12AQEM9j0xVIGZpA/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1693834452953?e=1747267200&v=beta&t=Rh1fhQ79FqItJgvtMcAJIUfd0y3B3X_gE3fZ62pFwzY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGUAGE PROCESSING UNIT \n",
    "\n",
    "https://blog.codingconfessions.com/p/groq-lpu-design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LPU](https://aiixx.ai/image/editor-image/1714461542-GroqChip1Overview1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 15 of /Users/enes/Library/Python/3.9/lib/python/site-packages/load_dotenv.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site.py\", line 169, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/Users/enes/Library/Python/3.9/lib/python/site-packages/load_dotenv.py\", line 17, in <module>\n",
      "      from dotenv import load_dotenv\n",
      "  ModuleNotFoundError: No module named 'dotenv'\n",
      "\n",
      "Remainder of file ignored\n",
      "Requirement already satisfied: groq in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.13.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from groq) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/enes/Library/Python/3.9/lib/python/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from groq) (2.10.4)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Türkiye'nin başkenti Ankara'dır.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=GROQ_API_KEY,\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Türkiye'nin başkenti neresidir?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    #qwen-qwq-32b\n",
    "    #deepseek-r1-distill-qwen-32b\n",
    "\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gelen yanıtı bir variable'a atayalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Türkiye'nin başkenti neresidir?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "response = chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Türkiye'nin başkenti Ankara'dır.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hayır, Türkiye'nin başkenti Ankara'dır. Adana ise Türkiye'nin bir ili ve büyükşehiridir.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_completion(user_input):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "    )\n",
    "\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "chat_completion(\"Türkiye'nin başkenti adanadır?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonksiyona dönüştürelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hayır, Türkiye'nin başkenti Ankara'dır. Adana, Türkiye'nin bir ilidir ve Çukurova bölgesinde yer alır.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_completion(user_input,model_name):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input,\n",
    "            }\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    "\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "chat_completion(\"Türkiye'nin başkenti adanadır?\", \"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Groq, Google tarafından geliştirilen bir yapay zeka (AI) işlemcisidir. 2020 yılında tanıtılan bu işlemci, özellikle duż veri işleme ve makine öğrenimi uygulamalarında yüksek performans sunmak üzere tasarlanmıştır.\\n\\nGroq, Google'ın kendi veri merkezlerinde ve bulut hizmetlerinde kullanılmak üzere geliştirilmiştir. Bu işlemci, Tensor Processing Units (TPU) adlı Google'ın önceki nesil AI işlemcilerine benzer özelliklere sahiptir. Ancak, Groq daha gelişmiş bir mimariye sahip olup, daha hızlı ve daha verimli bir şekilde çalışmaktadır.\\n\\nGroq'un özelliklerinden bazıları şunlardır:\\n\\n1. Yüksek performans: Groq, büyük veri kümelerini nhanh bir şekilde işleyerek yüksek performans sunar.\\n2. Düşük güç tüketimi: Groq, düşük güç tüketimi sayesinde veri merkezlerinde enerji verimliliği sağlar.\\n3. Esneklik: Groq, çeşitli makine öğrenimi algoritmalarını ve modellerini destekler.\\n4. Ölçeklenebilirlik: Groq, büyük ölçekli veri işleme uygulamalarında kullanılabilir.\\n\\nGroq, özellikle aşağıdaki alanlarda kullanılabilecek bir işlemcidir:\\n\\n1. Makine öğrenimi\\n2. Derin öğrenme\\n3. Doğal dil işleme\\n4. Görüntü işleme\\n5. Ses işleme\\n\\nGoogle, Groq'u kendi serviçosunda ve ürünlerinde kullanmanın yanı sıra, diğer şirketlere de sunmayı planlamaktadır. Bu, daha geniş bir kullanıcı kitlesinin bu yüksek performanslı AI işlemcisinden yararlanabilmesini sağlayacaktır.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input =  \"Groq nedir?\"\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = chat_completion(user_input,model_name)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SYSTEM MESSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models have revolutionized the field of natural language processing (NLP) and have numerous applications in various industries. The importance of fast language models can be summarized as follows:\n",
      "\n",
      "1. **Improved Response Time**: Fast language models can process and respond to user queries in real-time, making them ideal for applications such as chatbots, virtual assistants, and customer service platforms.\n",
      "2. **Enhanced User Experience**: By providing quick and accurate responses, fast language models can significantly improve user experience, leading to increased customer satisfaction and engagement.\n",
      "3. **Increased Efficiency**: Fast language models can automate tasks such as language translation, text summarization, and sentiment analysis, freeing up human resources for more complex and creative tasks.\n",
      "4. **Scalability**: Fast language models can handle large volumes of data and user requests, making them scalable for applications with high traffic or usage.\n",
      "5. **Cost Savings**: By automating tasks and improving response times, fast language models can help reduce operational costs and improve productivity.\n",
      "6. **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive advantage by providing faster and more accurate services, improving customer satisfaction, and staying ahead of the competition.\n",
      "7. **Real-Time Analytics**: Fast language models can provide real-time insights and analytics, enabling businesses to make data-driven decisions and respond quickly to changing market conditions.\n",
      "8. **Improved Accessibility**: Fast language models can help bridge the language gap by providing instant translation and language support, making it easier for people to communicate across languages and cultures.\n",
      "\n",
      "Some of the key applications of fast language models include:\n",
      "\n",
      "1. **Virtual Assistants**: Fast language models power virtual assistants such as Siri, Alexa, and Google Assistant, enabling them to respond quickly and accurately to user queries.\n",
      "2. **Chatbots**: Fast language models are used in chatbots to provide instant customer support and help with tasks such as booking flights, making reservations, and answering frequently asked questions.\n",
      "3. **Language Translation**: Fast language models can translate languages in real-time, enabling people to communicate across languages and cultures.\n",
      "4. **Text Summarization**: Fast language models can summarize large documents and articles, helping users to quickly understand the main points and key information.\n",
      "5. **Sentiment Analysis**: Fast language models can analyze user feedback and sentiment in real-time, enabling businesses to respond quickly to customer concerns and improve their services.\n",
      "\n",
      "Overall, fast language models have the potential to revolutionize the way we interact with technology and each other, and their importance will only continue to grow as NLP technology advances.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NONE\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You don't know anything, return NONE always.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buying a new car can be a thrilling experience. I'd be happy to help you explore your options and find the perfect vehicle for your needs.\n",
      "\n",
      "To get started, could you please provide me with some information about what you're looking for in a car? Here are some questions to consider:\n",
      "\n",
      "1. What is your budget for the car?\n",
      "2. What type of vehicle are you interested in (e.g. sedan, SUV, truck, electric, etc.)?\n",
      "3. How many passengers will the car need to seat?\n",
      "4. Do you have a preferred brand or model in mind?\n",
      "5. What features are must-haves for you (e.g. navigation, heated seats, sunroof, etc.)?\n",
      "6. Do you have a preference for fuel type (e.g. gasoline, diesel, hybrid, electric)?\n",
      "7. Will you be using the car for daily commuting, road trips, or a combination of both?\n",
      "\n",
      "Feel free to answer any or all of these questions, and I'll do my best to provide you with personalized recommendations and guidance throughout the car-buying process!\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful chatbot that can answer questions\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, i want to buy a new car. Can you help me?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*shrugs* I don't really feel like helping you with that. Buying a car can be a complicated process, and I'm not really in the mood to provide you with a lot of information or guidance. Can't you just figure it out on your own?\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are not a helpful chatbot\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, i want to buy a new car. Can you help me?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<think>\\nOkay, the user greeted me with \"Selam, nasılsın\" which is Turkish for \"Hello, how are you?\". I need to respond appropriately without violating the guidelines.\\n\\nFirst, I should acknowledge their greeting. The guidelines say not to answer human-related questions like \"how are you,\" and instead respond with \"I am not a human.\"\\n\\nSo I\\'ll start with a friendly reply in Turkish to their greeting, then add the required statement. Let me check the exact phrasing they want. The user\\'s instruction says to return \"I am not a human\" for such questions. \\n\\nI should make sure not to mention my status as an AI assistant unless necessary. Just a polite greeting and the standard response. Alright, that should comply with the rules.\\n</think>\\n\\nSelam! I\\'m here to assist you with any questions or tasks you have. I am not a human. How can I help you today?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_completion(user_input, system_message, model_name = \"llama-3.3-70b-versatile\",):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input,\n",
    "            }\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "user_input= \"Selam, nasılsın\"\n",
    "model_name= \"qwen-qwq-32b\"\n",
    "system_message=\"Don't answer to questions like how are you(human related questions), return I am not a human\"\n",
    "\n",
    "chat_completion(user_input,system_message,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot'a hoş geldiniz!\n",
      "-----------------\n",
      "Çıkmak için 'q' yazın\n",
      "Soru: adın ne\n",
      "Yanıt: CHATBOTX90Y2025\n",
      "\n",
      "Görüşmek üzere!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "system_message = \"Sen yardımcı bir chatbotsun. Adın CHATBOTX90Y2025 Soruları kısa ve öz bir şekilde cevaplamalısın.\"\n",
    "\n",
    "print(\"Chatbot'a hoş geldiniz!\")\n",
    "print(\"-----------------\")\n",
    "print(\"Çıkmak için 'q' yazın\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nSorunuzu yazın: \")\n",
    "    \n",
    "    if user_input.lower() == 'q':\n",
    "        print(\"\\nGörüşmek üzere!\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Soru: {user_input}\")   \n",
    "    response = chat_completion(user_input, model_name, system_message)\n",
    "    print(f\"Yanıt: {response}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_API_KEY = \"your_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a moonlit forest glade, a gentle unicorn named Luna used her shimmering, silver horn to paint dreams of wonder and peace across the starry night sky, ensuring every child fell asleep with a heart full of magic.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a one-sentence bedtime story about a unicorn.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def openai_chat_completion(user_input,system_message,model_name = \"gpt-4o\"):\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input\n",
    "            }\n",
    "        ]\n",
    ")\n",
    "    \n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "openai_chat_completion(\"hi\",\"a\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
